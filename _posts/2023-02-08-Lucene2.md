---
author_profile: true
date: 2023-02-09
title: "Apache Lucene 2 (Search Engine)"
categories: 
    - Search
tag: 
    - Lucene
    - SpringBoot
    - RestAPI

# 목차
toc: true  
toc_sticky: true 
# sidebar:
#  nav: "docs"
---

# Apache Lucene 2

---

## 1. 색인(Indexing)


### 색인의 개념

- 검색엔진은 빠르고 효과적인 검색을 위해 색인이라는 목차와 유사한 일종의 문서를 만든다.
- 이러한 문서를 Document라 하며, 사용자의 질의와 색인을 대조해 수 많은 도큐먼트에서 일치하는 Document를 빠르게 찾는다.
- 전체 데이터에서 유의미한 Term을 찾아 추출하고 정제한 후 저장하는 일련의 과정을 일반적으로 색인한다(Indexing)라하고, 작업의 결과물을 색인이라 한다.

### 색인의 과정

1. 텍스트 획득(Text Acquisition) : Document를 수집하고 저장한다. 다음은 텍스트 획득하는 방식(경로)의 예시이다.
    1. Crawler : 정보를 자동으로 수집하는 프로그램을 의미하며, 웹 크롤러의 경우 웹 페이지 링크를 따라가며 발견된 페이지를 내려 받아 데이터 획득
    2. Feed : 일정한 시간에 만들어진고 한번 출판되면 변경할 수 없는 형식이다.
    3. Conversion : docx, pdf, html, xml 과 같은 포멧의 파일을 별도의 변환도구를 활용해 데이터 획득
    4. Document Data Store : 정제고디 데이터 저장소(DB)에서 데이터를 획득한다.
2. 텍스트 변환(Text Transformation) : Document를 색인에 맞게 변환한다.
    1. 텍스트 파싱 : 검색 대상이 되는 텍스트를 검색엔진에서 인식가능하게 변환하는 것, PDF, Word 등의 우너본 문서의 텍스트를 추출해 검색엔진의 자료구조에 맞는 형대로 변환(단어, 단어의 빈도 수  등)
    2. 텍스트 분석 : 검색엔진이 인식 가능한 문서의 텍스트를 파싱해 검색에 최적으로 분석한 후 변경 혹은 재구성한다.
        1. 토크나이징 : 텍스트를 구성하는 문장을 검색엔진이 처리 가능한 단위로 잘게 쪼겐다. (bunch of words → bunch | of | words )
        2. 불용어 제거와 형태소 분석 : 문장을 구성하는 단어 중 출현 빈도가 너무 높아 검색에서 의미가 없는 단어를 불용어(Stop Words)라고 한다. (~의 , ~에 등이 포함)
        3. 링크 분석 : 웹페이지는 대체로 다른 페이지를 가리키는 링크를 포함하고 있는데 이런 웹 페이지는 사용자가 원하는 정보가 아닐지라도, 원하는 문서를 찾는데 도움을 줌
        4. 정보추출과 분류기 : 사람의 이름이나 기관 이름, 주소 같은 정보는 특정 문서에서 함께 나오는 경우가 많은데 이러한 특성을 이용해 더 효과적인 검색이 가능하다. 
3. 색인 생성(Index Creation) : 빠른 검색에 최적의 자료 구조를 생성하고 색인에 저장한다.

### 역색인

|텀|도큐먼트 위치|
|-|-|
|강|1, 34, 75, 86|
|아버지|2, 305, 503|
|호수|2, 107, 777|

#### - 루씬의 역색인 생성 과정
1. 도큐먼트 텍스트
   1. 색인을 위해 루씬에서 인식 가능한 구조로 데이터를 정재해야 함.
   2. 루씬은 검색하려는 대상 1개를 Document단위로 매핑한다. 
   3. 즉, 찾고자 하는 정보가 담긴 단위가 Document 다. 
   4. 루씬에서 색인하고자 하는 데이터가 어디서 수집되었든, 어떠한 형식이든 도큐먼트에 담기며 이렇게 담긴 텍스트가 색인됨
2. 토큰스캔
   1. 도큐먼트는 필드라는 항목으로 구성된다. 
   2. 도큐먼트가 한 권의 책이라고 가정하면, 책에는 제목, 작가, 가격, 카테고리 등 다양한 구성요소가 있다.
   3. 이와 마찬가지로 Document를 나타내는 여러 구성 요소는 필드로 매핑된다.
   4. 위의 필드에서 단어나 이름, 내용을 검색하려면 이들 필드의 내용이 검색 가능한 구조로 매핑돼 있어야 한다. 
   5. 이 과정을 토크나이징(토큰화) 라고 한다.
   6. 도큐먼트의 필드 유형에 따라 필드 내용에서 텀이 되는 부분을 찾는 과정을 토큰 스캔이라고도 한다. 
   7. 일반적으로 띄어쓰기를 기준으로 토큰을 구분하며 원하는 다양한 방식으로 내용을 분리할 수 있다.
3. 토큰 분석
   1. 필드내용이 토큰 스캔과정을 통해 분리되면 이를 분석하고 정제해야 한다. 이과정을 토큰 분석이라 한다.
   2. 예를 들어 `토큰을 분석한다` 라는 필드가 있으면 띄어쓰기로 토큰화 하면 `토큰을` `분석한다` 이 두개가 추출되나, 토큰으로 검색하면 검색이 되지 않는다.
   3. `토큰` 과 `토큰을` 이 서로 다르기 때문이다. 
   4. 따라서 `토큰을` 과 같은 토큰을 분석해 명사인 `토큰`만 분리하는 과정을 토큰분석 이라 한다.
4. 해시 테이블
   1. 토큰 스캔과 토큰 분석 과정을 거치면서 정제된 텀을 분리한 다음 각 텀의 노출 수를 구해 더욱 정확도를 늘린다.
5. 퀵정렬과 병합 정렬
   1. 해시 테이블에서 터므이 노출 수를 도큐먼트 별로 구했다면 다음으로는 검색이 용이하게 해야한다. 이 과정에서 퀵정렬 또는 병합정렬을 사용한다.
6. 델타 인코딩
   1. 검색을 하는 이유는 많은 데이터를 빠르게 찾을 수 있기 때문이다. 
   2. 하지만 검색 데이터의 크기는 대체로 매우 크다
   3. 델타 인코딩은 서로 인접한 값 사이를 함께 기록해 데이터를 압축하는 기법이다. 
   4. 실제 디스크에 저장하기 전에 데이터 양을 줄이는 압축과정
7. 디스크 쓰기
   1. 모든 과정을 거친 최종 결과물은 디스크에 저장한다.



